---
title: "Kaggle-Predicting Sales"
output:
  html_document:
    css: "https://thebudgetactuary.github.io/Exam_SRM/style.css"
    number_sections: true
    toc: true
    toc_depth: 3
---

# Introduction

The goal of this study is to predict the future sales for fifty-four different Ecuadorian-based grocery stores of the Corporacian Favorita retail chain. Doing this will allow the retail to prepare for the expected future sales by stocking the shelves with inventory accordingly. 

# Libraries

```{r libraries, warning=FALSE,message=FALSE}
library(dplyr)
library(ggplot2)
library(gt) #neat tables
library(lubridate) #time series functions
library(stringr) #string functions
library(gridExtra) #organizing plots
library(TSstudio) #time series functions like ts_decompose
library(xts) # ts data frame
library(imputeTS) 
library(caret) # modeling
library(Metrics) #rmsle
options(scipen = 999) # removes scientific notation
```

# Importing Data

The data used for this study comes from six different data frames. For time sake, holidays will be excluded from this study.

```{r importing data}
oil <- read.csv("oil.csv",stringsAsFactors = FALSE)
stores <- read.csv("stores.csv",stringsAsFactors = FALSE)
transactions <- read.csv("transactions.csv",stringsAsFactors = FALSE)
train <- read.csv("train.csv",stringsAsFactors = FALSE)
test <- read.csv("test.csv",stringsAsFactors = FALSE)
# Holidays is excluded from this study for times sake
```


## Oil

The oil data set contains 1218 observations and 2 variables:

__date:__ Ranges from January 1st, 2013 to August 31, 2017.

__dcoilwtico:__ The daily oil price for that day.

```{r oil}
gt(head(oil))
```

## Stores

The stores data set contains 54 observations and 5 variables:

__store_nbr:__ Specific store number ranging from one to fifty-four.

__city:__ City where store is located.

__state:__ State where store is located.

__type:__ Type of store grouped into five categories: A, B, C, D, or E.

__cluster:__ Grouping of similar stores, seventeen possible groups.

```{r stores}
gt(head(stores))
```


## Transactions

The transactions data set contains 83,488 observations and 3 variables:

__date:__ Ranging from January 1st, 2013 to August 15th, 2017.

__store_nbr:__ The unique identifier for store number.

__transactions:__ The number of transactions made.

```{r transactions}
gt(head(transactions))
```

## Train

The train data set contains 3,000,888 observations and 6 variables:

__id:__ Unique id for each observation.

__date:__ Ranging from January 1st, 2013 to August 15th, 2017.

__store_nbr:__ fifty-four stores.

__family:__ Deparment in store. 

__sales:__ Daily sales for each family.

__onpromotion:__ Total number of products in a family that were being promoted that day.


```{r train}
gt(head(train))
```


## Test

The test set is where our final predictions will be made. Contains 28,512 observations and 5 variables:

__id:__ Unique identifier.

__date:__ Ranging from August 16th, 2017 to August 31st, 2017.

__store_nbr:__ Fifty-four stores.

__family:__ Department of store, thirty-three possible values.

__onpromotion:__ Number of items on promotion for a given family for that day.


```{r test}
gt(head(test))
```

## Combining Datasets

In order for easier data analysis the data sets will be combined.

```{r combining train}
train <- left_join(train,transactions,by=c("date"="date","store_nbr"="store_nbr"))

train <- left_join(train,stores,by=("store_nbr"="store_nbr"))

train <- left_join(train,oil,by=("date"="date"))


train <- train %>% select(-id)

```


```{r combining test}
test <- left_join(test,stores,by=("store_nbr"="store_nbr"))

test <- left_join(test,oil,by=("date"="date"))

```


# Data Analysis

In order to get the best results from our data we must first understand our data. In this section we will begin breaking down our data to understand what must be accomplished in our feature engineering section.

## NA Values

From the data only the oil prices and transactions variables contain NA values. This could represent days where the oil prices were not recorded and days when no transactions were made. Since transactions is not available in the test set, it will not used for our forecasting. Imputation will be used for oil prices in the feature engineering section.

```{r na counts}
nadf <- data.frame(sapply(train,function(x) sum(is.na(x))))
nadf$variables <- row.names(nadf)
names(nadf)[1] <- "naCount"

ggplot(nadf,aes(x=variables,y=naCount,fill=variables))+
  geom_col()+
  labs(title="NA Count of Train Data")+
  ylab("NA Count")+
  theme(legend.position="none")

rm(nadf)
```


## Sales

The sales seem to have a positive trend 
across every year. There is a seasonal pattern with December having the highest sales every year.

```{r sales over time, message=FALSE, warning=FALSE}

train %>% 
  group_by(month=month(date),year=year(date)) %>% 
  filter(year %in% c(2013,2014,2015,2016)) %>% 
  summarize(monthlySales=sum(sales)) %>% 
  ggplot(.,aes(x=month,y=monthlySales))+
  geom_line(aes(color=as.factor(year)))+
  labs(title="Total Monthly Sales",
       subtitle="from January 2013 to December 2016")+
  xlab("Month")+
  ylab("Monthly Sales")+
  scale_x_continuous(breaks = seq(1, 12, by = 1))+
  guides(color=guide_legend(title="year"))

```

On Kaggle it mentioned that the first and fifteenth of every month was payday. It appears there might be some relationship with average sales and the day of the month.

```{r day of month}
train %>% group_by(dayOfWeek=day(date)) %>% 
  summarize(sales=mean(sales)) %>% 
  ggplot(.,aes(x=dayOfWeek,y=sales))+
  geom_col()
```



Weekends have higher average sales than week days.

```{r daily seasonality, message=FALSE, warning=FALSE}
train %>% 
  mutate(dayOfWeek=wday(ymd(date),label=TRUE)) %>% 
  group_by(dayOfWeek) %>% 
  summarize(totalSales=mean(sales)) %>% 
  ggplot(.,aes(x=as.factor(dayOfWeek),y=totalSales))+
  geom_col(aes(fill=dayOfWeek))+
  xlab("Day of Week")+
  ylab("Average Sales")
  

```


## Predictors

__Store Number__

There were a total fifty-four stores. Some appeared to not be open at the start of 2013 as the sales in earlier years for some of the stores is zero. All stores seemed to have increasing sales each year. Only half of the year of 2017 was recorded explaining why the total sales were lower for 2017 across all stores. Finally, the sales at some stores were significantly greater than some other stores.

```{r store number}
for(i in row.names(table(train$store_nbr))){
  group <- train[train$store_nbr==as.integer(i),c("date","store_nbr","sales")] %>% 
    group_by(year=year(date)) %>% 
    summarize(yearlySales=sum(sales))
    
    print(ggplot(group,aes(x=year,y=yearlySales))+
            geom_col()+
            labs(title=paste("Store Number: ",i))+
            ylab("Yearly Sales")+
            ylim(c(0,16400000)))
}

```

__Family__

The family variable has thirty-three different departments. Each department has a different sales pattern so will be modeled separately. There is a big gap after 2015 for some departments, this could affect the modeling stage. 

```{r family total sales}

for(name in row.names(table(train$family))){
  temp <- train %>% 
    filter(family==name)
  
  print(ggplot(temp,aes(x=ymd(date),y=sales))+
          geom_line()+
          labs(title=name))
}

train %>% 
  group_by(family) %>% 
  summarize(totalSales=sum(sales)) %>% 
  arrange(desc(totalSales)) %>% 
  head(8) %>% 
  ggplot(.,aes(x=reorder(family,desc(totalSales)),y=totalSales,fill=family))+
  geom_col()+
  labs(title="Top 8 Selling Families")+
  xlab("Families")+
  ylab("Total Sales")

rm(familyTotals)
```

__Promotion__

All stores that had at least one promotion on a given day had higher average sales than if they did not have a promotion. It also appears the number of promotions on a given day lead to higher sales up to 250 promotions, and then there appears to be a sharp decrease in average sales. Nearly all these instances of over 250 promotions occurred in stores 53 and 54.

```{r promotion charts, warning=FALSE, message=FALSE}
train %>%
  group_by(onpromotion) %>% 
  summarize(meanSales=mean(sales)) %>% 
  ggplot(.,aes(x=onpromotion,y=meanSales))+
  geom_line()+
  xlab("promotion count")+
  ylab("average sales")+
  labs(title="Number of Promotions vs Average Sales")

train %>% 
  mutate(hasAPromotion=ifelse(onpromotion>0,1,0)) %>% 
  group_by(hasAPromotion,store_nbr) %>% 
  summarize(meanSales=mean(sales)) %>% 
  ggplot(.,aes(x=store_nbr,y=meanSales,fill=as.factor(hasAPromotion)))+
  geom_col(aes(position="dodge"))+
  scale_fill_discrete(labels=c("at least 1 promotion","no promotion"))+
  labs(fill="promotion",
       title="Daily Sales",
       subtitle="With and Without a Promotion")+
  xlab("store number")+
  ylab("mean daily sales")


print("Instances of over 250 daily promotions by store number")
table(as.data.frame(train %>%
  group_by(onpromotion,store_nbr) %>% 
  summarize(meanSales=mean(sales)) %>% 
  filter(onpromotion>250))$store_nbr)

```

__Transactions__

There is a rather weak correlation between the number of transactions and amount of sales. This could be caused from large differences in the price of the items.

```{r transactions plots, message=FALSE, warning=FALSE}
train %>%
  group_by(year=year(date),month=month(date)) %>% 
  summarize(monthlySales=sum(sales),monthlyTransactions=sum(transactions,na.rm=TRUE)) %>% 
  mutate(date=ym(str_c(year,",",month))) %>% 
ggplot(.,aes(x=monthlyTransactions,y=monthlySales))+
  geom_point()+
  geom_smooth(method="lm",se=FALSE)+
  xlab("monthly transactions")+
  ylab("monthly sales")+
  labs(title="Monthly Sales vs Transactions")
  

cor(train$sales,train$transactions,use="complete.obs")

train %>%
  filter(!is.na(transactions)) %>% 
  group_by(month=month(date),year=year(date)) %>% 
  summarize(monthlyTransactions=sum(transactions)) %>% 
  mutate(date=ym(str_c(year,",",month))) %>% 
  ggplot(.,aes(x=date,y=monthlyTransactions))+
  geom_line()+
  xlab("Date")+
  ylab("Monthly Transactions")+
  labs(title="Monthly Transactions",
       subtitle="from January 2013 to August 2017")

```


When comparing the transactions vs the sales by individual families we can see that some families have a very high correlation between transactions and sales.  

```{r transactions vs sales by family}
corlist <- list()
for(name in names(table(train$family))){
  temp <- train[train$family==name,] #filter by family
  
  tempCor <- cor(temp[,"transactions"],temp[,"sales"],use="complete.obs") #cor of trans. and sales for each family
  
  corlist <- append(corlist,tempCor) #saving cor results
}

byFamily <- data.frame(family=names(table(train$family)),
           correlation=unlist(corlist))


byFamily %>% 
  arrange(desc(correlation)) %>% 
  head(15) %>% 
ggplot(.,aes(x=reorder(family,desc(correlation)),y=correlation)) + 
  geom_col(aes(fill=family))+
  ylab("correlation between transactions and sales")+
  xlab("family")+
  labs(title="Correlation of Transactions vs Sales",
       subtitle="Grouped by Family, Top 15")+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
        legend.position="none")

```



__City__ 

There are a total of 22 different cities in the data set. Quito has the largest amount of sales but also contains the largest amount of stores. Most cities contain only 1 store.


```{r city}
totalCity <- train %>% 
  group_by(city=city) %>% 
  summarize(sales=sum(sales)) %>% 
  ggplot(.,aes(x=reorder(city,desc(sales)),y=sales,fill=city))+
  geom_col()+
  xlab("city")+
  labs(title="Sales per City")+
  
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
        legend.position="none")

countCity <- train %>% 
  group_by(city) %>% 
  summarize(count=n()) %>% 
  mutate(count=count/55572) %>% 
  arrange(desc(count)) %>% 
  ggplot(.,aes(x=reorder(city,desc(count)),y=count,fill=city))+
  geom_col()+
  xlab("city")+
  ylab("count")+
  labs(title="Number of Stores per City")+
  geom_text(aes(label = count), vjust = 0)+
  theme(legend.position="none",
        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

grid.arrange(totalCity,countCity,ncol=2)
```

__State__

The stores exist in 16 different states. Pichincha contains 19 of the total 54 stores and also has the highest total sales. 

```{r state}

salesState <- train %>% 
  group_by(state) %>% 
  summarize(totalSales=sum(sales)) %>% 
  ggplot(.,aes(x=reorder(state,desc(totalSales)),y=totalSales,fill=state))+
  geom_col()+
  xlab("state")+
  ylab("total sales")+
  labs(title="Number of Sales per State")+
  theme(legend.position="none",
        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

countState <- train %>% 
  group_by(state) %>% 
  summarize(count=n()/55572) %>% 
  ggplot(.,aes(x=reorder(state,desc(count)),y=count,fill=state))+
  geom_col()+
  geom_text(aes(label = count), vjust = 0)+
  xlab("state")+
  labs(title="Number of Stores per State")+
  theme(legend.position="none",
        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

grid.arrange(salesState,countState,ncol=2)
```


__Type__

The type of stores labeled "A" have the largest average sales of the 5 different possible types of stores. Most stores are type "D" or type "C". 

```{r type}
salesType <- train %>% group_by(type) %>% 
  summarize(meanSales=mean(sales)) %>% 
  ggplot(.,aes(x=type,y=meanSales,fill=type))+
  geom_col()+
  ylab("average sales")+
  labs(title="Average Sales per Store Type")

countType <- train %>% 
  group_by(type) %>% 
  summarize(count=n()/55572) %>% 
  ggplot(.,aes(x=type,y=count,fill=type))+
  geom_col()+
  geom_text(aes(label = count), vjust = 0)+
  labs(title="Count of Store Type")

grid.arrange(salesType,countType,ncol=2)
```



__Cluster__

Stores that are type "D" are all from the same cluster of stores. Stores of type "D" contain 6 different types of store clusters. Cluster 14 only contains 4 stores and outsells the clusters with 6 or 7 stores.

```{r cluster, message=FALSE, warning=FALSE}
train %>% 
  group_by(type,cluster) %>% 
  summarize(count=n()/55572) %>% 
  ggplot(.,aes(x=type,y=count,fill=as.factor(cluster)))+
  geom_col(position="fill")+
  ylab("percentage of cluster")+
  xlab("type")+
  labs(title="Percentage of Clusters in Each Type")+
  theme(legend.position="none")

train %>% 
  group_by(cluster) %>% 
  summarize(totalSales=sum(sales),
            count=n()/55572) %>% 
  ggplot(.,aes(x=reorder(cluster,desc(totalSales)),y=totalSales,fill=as.factor(cluster)))+
  geom_col()+
  geom_text(aes(label = count), vjust = 0)+
  xlab("cluster")+
  ylab("total sales")+
  labs(title="Total Sales per Cluster",
       subtitle="With Counts of Stores in that Cluster")+
  theme(legend.position="none")

```


__Oil Prices__

From the plots we can see a strong negative correlation between the oil prices and monthly sales. The correlation is -.78 suggesting that as oil prices increase we can see higher sales. 

```{r df of sales vs oil, warning=FALSE,message=FALSE}
monthlyOilVsSales <- train %>% 
  group_by(date) %>% 
  summarize(dailySales=sum(sales,na.rm=TRUE)) %>% 
  left_join(oil,by=c("date"="date")) %>% 
  group_by(month=month(date),year=year(date)) %>%
  summarize(monthlySales=sum(dailySales),
            avgMonthlyOilPrice=mean(dcoilwtico,na.rm=TRUE)) %>% 
  mutate(monthlyDate=ym(str_c(year,",",month))) %>% 
  filter(monthlyDate<ymd(str_c("2017",",","08",",","1")))

totalMonthlySales <- ggplot(monthlyOilVsSales,aes(x=monthlyDate,y=monthlySales))+
  geom_line()+
  labs(title="Monthly Sales")+
  xlab("date")+
  ylab("total monthly sales")
averageOilPrices <- ggplot(monthlyOilVsSales,aes(x=monthlyDate,y=avgMonthlyOilPrice))+
  geom_line()+
  labs(title="Average Monthly Oil Prices")+
  ylab("monthly oil price")+
  xlab("date")

grid.arrange(totalMonthlySales,averageOilPrices,nrow=2)

print(paste("The correlation between sales and oil prices is:",round(cor(monthlyOilVsSales$monthlySales,monthlyOilVsSales$avgMonthlyOilPrice),2)))
```


## Key Findings:

- Imputation needs to be used for oil prices as the price is not recorded everyday.

- Sales exhibit a positive trend over time.

- Some relationship between day of week and average sales.

- There is a monthly seasonal effect on sales.

- Sales varies widely between stores.

- Each family had a different sales pattern.

- Sales increase when a promotion is present.

- Within families there is a strong correlation between transactions and sales.

- The type of store has a large effect on average sales.

- Oil prices and sales have a strong negative correlation.


# Feature Engineering 

## Oil Imputation

The na_kalman function from the imputeTS package will be used for imputation on the oil prices.

```{r imputing oil prices}

oilPrices <- train %>% 
  group_by(date) %>% 
  summarize(oilPrices=mean(dcoilwtico))

imputedOil <- na_kalman(oilPrices$oilPrices)

ggplot_na_imputations(x_with_na = oilPrices$oilPrices,x_with_imputations = imputedOil)

imputedWDate <- data.frame(date=oilPrices$date,imputedOil=imputedOil)

train <- left_join(train,imputedWDate,by=c("date"="date"))

train <- train %>% select(-dcoilwtico)

```


```{r imputing oil2}
oilPricesTest <- test %>% 
  group_by(date) %>% 
  summarize(oilPrices=mean(dcoilwtico))

imputedOilTest <- na_kalman(oilPricesTest$oilPrices)

imputedWDateTest <- data.frame(date=oilPricesTest$date,imputedOil=imputedOilTest)

test <- left_join(test,imputedWDateTest,by=c("date"="date"))

test <- test %>% select(-dcoilwtico)

```

## Trend

The variables of trend and trend_sq will be created. Trend will be used to address the positive trend of our sales over time. Trend_sq will be used to address if this relationship is not linear. 


```{r creating trend and trend sq}
train$trend <- as.numeric(ymd(train$date))-15705
train$trend_sq <- train$trend^2

test$trend <- as.numeric(ymd(test$date))-15705
test$trend_sq <- test$trend^2
```

## Seasonal Effect

Month predictor is added to address monthly seasonality. We saw earlier that later months such as December displayed the highest average sales.

```{r creating monthly effect}
train$month <- month(train$date,label = TRUE)

test$month <- month(test$date,label = TRUE)

```

## Pay-Day

Predictor given a value of '1' if it is payday or '0' otherwise.

```{r payday predictor}
train <- train %>% 
  mutate(payday=ifelse(day(date) %in% c(1,15),1,0))

test <- test %>% 
  mutate(payday=ifelse(day(date) %in% c(1,15),1,0))
```


## Weekend

The wday variable will address whether it is a weekday or a weekend.

```{r edittttttttttt}
train <- train %>% 
  mutate(wday=ifelse(wday(date) %in% c(2,3,4,5,6),"weekday","weekend"))

test <- test %>% 
  mutate(wday=ifelse(wday(date) %in%
                       c(2,3,4,5,6),"weekday","weekend"))
```

## Sales vs No Sales

In some stores certain families made zero sales. These will be separated out from the rest of our data and will be given a prediction of zero in the end. 

```{r nosalesdf, warning=FALSE, message=FALSE}
noSales <- as.data.frame(train %>% 
  group_by(family,store_nbr) %>% 
  summarize(count=sum(sales==0)) %>% 
  filter(count==1684))


hasSales <- as.data.frame(train %>% 
  group_by(family,store_nbr) %>%
  summarize(count=sum(sales==0)) %>% 
    filter(count<1684))

```

Some stores made sales within a deparment but not recently. Any combination of stores and families that did not make a sale in the past 365 days will be added to the no sales data frame.

```{r no sales past year}

for(i in 1:nrow(hasSales)){
  temp <- train %>% 
    filter(family==hasSales[i,1],store_nbr==hasSales[i,2])
  if(sum(temp$sales[1320:1684])==0){
  print(paste(i,hasSales[i,1],hasSales[i,2]))
  }
}
```

```{r adding to the data frame}
noSalesRows <- c(66,1040,1042,1043,1044,1045,1046,1048,1051,1060,1061,1063)

noSales <- rbind(noSales,hasSales[noSalesRows,])

hasSales <- hasSales[-noSalesRows,]

noSales <- noSales %>% 
  arrange(family,store_nbr)

hasSales <- hasSales %>% 
  arrange(family,store_nbr)



```

The zero cutoff function will be used to address families that did not start selling when the data started being recorded. Any family sections that made 0 sales in a store for the first 365 days will be assigned an NA. This will allow us to ignore those observations when we get to the modeling stage.

```{r zerocutoff function}
zeroCutOff <- function(myVector){
  j=1
    while(sum(myVector[j:min(j+100,length(myVector))])==0){
      j=j+100
    }
  
    while(myVector[j+1]==0){
      j=j+1
    }
    return(j)
}
```


```{r set no early sales to na values}
for(i in 1:nrow(hasSales)){

  if(sum(train[train$family==hasSales[i,"family"] &
         train$store_nbr==hasSales[i,"store_nbr"],"sales"][1:365])==0){
    
    cutOff <- zeroCutOff(train[train$family==hasSales[i,"family"] &
         train$store_nbr==hasSales[i,"store_nbr"],"sales"])
    
    train[train$family==hasSales[i,"family"] &
         train$store_nbr==hasSales[i,"store_nbr"],"sales"][1:cutOff] <- NA
    
  }
}
```

## Outliers

Grocery 1 has an outlier value in store fifty-four. When locating this value it did not display anything special so will be replaced with the average of the two nearest values of sales for that store.

```{r Grocery I}
train %>% filter(family=="GROCERY I") %>% 
  group_by(trend) %>% 
  summarize(sales=sum(sales,na.rm=TRUE)) %>% 
  arrange(desc(sales)) %>% 
  head(10)


train %>% filter(trend %in% c(1203,1204,1205),store_nbr==45,family=="GROCERY I") 

train[train$store_nbr==45 & train$trend==1204 & train$family=="GROCERY I","sales"] <- (16447+15282)/2 # set to the average of the sales of the two surrounding days

```

Meat family had an outlier in store 39. The same approach will be used of averaging the sales values from the previous and next day.

```{r Meat}
train %>% filter(family=="MEATS") %>% 
  group_by(trend) %>% 
  summarize(sales=sum(sales,na.rm=TRUE)) %>% 
  arrange(desc(sales)) %>% 
  head(10)

train %>% filter(trend%in%c(1375,1376,1377),family=="MEATS",store_nbr==39)

train[train$family=="MEATS" & train$store_nbr==39 & train$trend==1376,"sales"] <- (223.671+209.094)/2
```

## Correction for Modeling

During the modeling stage there was a problem with the ladieswear family in store 13 and 14. This is likely due to the gap that was present in the sales. The data before this gap will be ignored and set to NA values.

```{r ladieswear fix, warning=FALSE}
train %>% filter(family=="LADIESWEAR",store_nbr==13) %>% 
ggplot(.,aes(x=trend,y=sales))+
  geom_line()

train %>% filter(family=="LADIESWEAR",store_nbr==13) %>% 
  filter(trend>875) %>% 
  select(trend,store_nbr,family,sales) %>% 
  head(10)

train[train$trend<=879 & train$store_nbr==13 & train$family=="LADIESWEAR","sales"] <- NA


train %>% filter(family=="LADIESWEAR",store_nbr==14) %>% 
ggplot(.,aes(x=trend,y=sales))+
  geom_line()

train %>% filter(family=="LADIESWEAR",store_nbr==14) %>% 
  filter(trend>1575) %>% 
  select(trend,store_nbr,family,sales)

train[train$trend<=1580 & train$store_nbr==14 & train$family=="LADIESWEAR","sales"] <- NA

```

## Dropping Variables

The effects of the date variable is being replaced with the trend features. Transactions only exists in the train set and not the test set. Since we will model each family and store number combination independently they will exhibit the same city, state, type, and cluster in each model so will be dropped.

```{r dropping unneeded}
head(train)
train <- train %>% 
  select(-date,-transactions,-city,-state,-type,-cluster)

test <- test %>% 
  select(-date,-city,-state,-type,-cluster)

```

# Pre-Processing

Strings will be converted to factors.

```{r strings to factors}

train$month <- as.character(train$month)
test$month <- as.character(test$month)

for(name in names(train)){
  if(is.character(train[,name])){
    train[,name] <- as.factor(train[,name])
  }
}

for(name in names(test)){
  if(is.character(test[,name])){
    test[,name] <- as.factor(test[,name])
  }
}
```

## Dummy Vars

```{r dummy vars}
head(train)
makeDummy <- dummyVars(~month+wday,data=train,fullRank = TRUE)
train2 <- predict(makeDummy,train)
train2 <- as.data.frame(train2)
train <- cbind(train[,-c(8,10)],train2)

test2 <- predict(makeDummy,test)
test2 <- as.data.frame(test2)
test <- cbind(test[,-c(8,10)],test2)

rm(train2,test2)


```

## Standardization

```{r standardize}
head(train)
makeStandard <- preProcess(train[,c(4,5,6,7)],method=c("center","scale"))
train <- predict(makeStandard,train)

test <- predict(makeStandard,test)

gt(head(train))

```




```{r eval=FALSE}
write.csv(train,"trainCleaned.csv",row.names = FALSE)
write.csv(test,"testCleaned.csv",row.names = FALSE)
```

```{r eval=FALSE}
train <- read.csv("trainCleaned.csv")
test <- read.csv("testCleaned.csv")
```

# Modeling

The train control method used in our modeling phase is formed below. It uses the custom summary of rmsle which is the objective metric of Kaggle.

```{r summary function}
custom_summary = function(data, lev = NULL, model = NULL) {
out = Metrics::rmsle(data[, "obs"], data[, "pred"])
names(out) = c("rmsle")
out
}


ctrl <- trainControl(method="cv",
                     number=5,
                     summaryFunction = custom_summary)
```

## Elastic Net

The way the following for loop works is it loops over all the combinations of families and store numbers from the hasSales data frame. This is the data frame that contains all the combinations that we will not predict as zero.

At each iteration it will build an enet model and add it to a list. The value printed will be the average rmsle for all store numbers for each family present in the hasSales data frame.


```{r model1, warning=FALSE, eval=FALSE}
rm(enetModels)
enetModels <- list()
oneResult <- 0
results <- 0
counter <- 0

for(i in 1:nrow(hasSales)){
    families <- hasSales[i,1]
    stores <- hasSales[i,2]
  
    newTemp <- train[complete.cases(train),] %>% 
      filter(family==families & store_nbr==stores)
    
    set.seed(147)
    enetMod <- train(sales~.,
                   data=newTemp[,-c(1,2)],
                   method="glmnet",
                   trControl=ctrl,
                   metric="rmsle",
                   maximize=FALSE)
    
    enetModels[[i]] <- enetMod
    oneResult <- min(enetMod$results$rmsle,na.rm=TRUE)
    results <- results+oneResult
    counter <- counter+1
    
    if(i==nrow(hasSales)){
      print(paste(families,round(results/counter,4)))
    }
    
    if(i!=nrow(hasSales) & families!=hasSales[i+1,1]){
      print(paste(families,round(results/counter,4)))
      counter <- 0
      results <- 0
    }
}
```


```{r saving model1, warning=FALSE, eval=FALSE}
library(rlist)
rlist::list.save(enetModels,"enetModels1.rds")
```

```{r loading lists}
enetModels <- rlist::list.load("enetModels1.rds")
```

```{r}
modelingResults <- function(hasSalesDF){
  oneResult <- 0
  results <- 0
  counter <- 0
for(i in 1:nrow(hasSalesDF)){
  families <- hasSales[i,1]
    stores <- hasSales[i,2]
    
    oneResult <- min(enetModels[[i]]$results$rmsle,na.rm=TRUE)
    results <- results+oneResult
    counter <- counter+1
    
    if(i==nrow(hasSalesDF)){
      print(paste(families,round(results/counter,4)))
    }
    
    if(i!=nrow(hasSalesDF) & families!=hasSales[i+1,1]){
      print(paste(families,round(results/counter,4)))
      counter <- 0
      results <- 0
    }
}
}

modelingResults(hasSales)
```






# Feature Engineering 2

Since produce performed extremely poorly and it was one of the top selling families changes will be made to that family. There exists a weird gap where sales were not present across all stores. In order to address this all sales before this gap will be ignored by setting their values to NA.


```{r fixing produce}
hasSales$nrow <- seq(1,nrow(hasSales))
hasSales %>% filter(family=="PRODUCE")


train %>% filter(family=="PRODUCE") %>% 
  ggplot(.,aes(x=trend,y=sales))+
  geom_line()

train %>% filter(family=="PRODUCE") %>% 
  filter(trend>.07) %>% select(trend,family,store_nbr,sales) %>% 
  head(10)

train[train$family=="PRODUCE" & train$trend<.076,"sales"] <- NA 

```

# Modeling 2

New models will added to the enetModels list which contains all of our models in order to replace the produce models.

```{r new models added, eval=FALSE}
for(i in 1556:1609){
    families <- hasSales[i,1]
    stores <- hasSales[i,2]
  
    newTemp <- train[complete.cases(train),] %>% 
      filter(family==families & store_nbr==stores)
    
    set.seed(147)
    enetMod <- train(sales~.,
                   data=newTemp[,-c(1,2)],
                   method="glmnet",
                   trControl=ctrl,
                   metric="rmsle",
                   maximize=FALSE)
    
    enetModels[[i]] <- enetMod
    oneResult <- min(enetMod$results$rmsle,na.rm=TRUE)
    results <- results+oneResult
    counter <- counter+1
    
    if(i==nrow(hasSales)){
      print(paste(families,round(results/counter,4)))
    }
    
    if(i!=nrow(hasSales) & families!=hasSales[i+1,1]){
      print(paste(families,round(results/counter,4)))
      counter <- 0
      results <- 0
    }
}
```



```{r saving list, eval=FALSE, warning=FALSE}
rlist::list.save(enetModels,"enetModels2.rds")
```

```{r loading list, eval=TRUE}
enetModels <- rlist::list.load("enetModels2.rds")
```

```{r}
modelingResults(hasSales)
```


## Forming Predictions

Predictions will be made from each of our models on their respective store number and family in the test set.

```{r predictions2, warning=FALSE}
rm(testGroups)
testGroups <- list()

for(i in 1:nrow(hasSales)){
  
     tempTest <- test %>% 
      filter(family==hasSales[i,1] & store_nbr==hasSales[i,2])
    
    testGroups[[i]] <- data.frame(id=tempTest$id,predict(enetModels[[i]],tempTest))
  
  i=i+1
}
length(testGroups)

```

These predictions will be made into a data frame.

```{r unlist, warning=FALSE}
rm(unlisted)
unlisted <- unlist(testGroups)
output <- data.frame(id=unlisted[grepl("id",names(unlisted))],sales=unlisted[!grepl("id",names(unlisted))])

head(output)
```

All family and store number combinations that did not make a sale will be given a prediction of 0.

```{r zero predictions, warning=FALSE}

rm(testGroupsZeros)
testGroupsZeros <- list()

for(i in 1:nrow(noSales)){
  
     tempTest <- test %>% 
      filter(family==noSales[i,1] & store_nbr==noSales[i,2])
    
    testGroupsZeros[[i]] <- data.frame(id=tempTest$id,sales=0)
  
  i=i+1
}
length(testGroupsZeros)

```

The sales predictions and zero predictions will be combined and arranged by id.

```{r unlisted2, warning=FALSE}
rm(unlisted2)
unlisted2 <- unlist(testGroupsZeros)
output2 <- data.frame(id=unlisted2[grepl("id",names(unlisted2))],sales=unlisted2[!grepl("id",names(unlisted2))])

finalOutput <- rbind(output,output2)
finalOutput <- finalOutput %>% 
  arrange(id)

```

# Submission

The analysis is complete! All that is left to do is save the csv just like below. 

```{r submission, eval=FALSE}

write.csv(finalOutput,file = "enetSubmission2.csv",row.names=FALSE)

```

Rooms for Improvement:

- Just like how produce was given its own special care after analyzing how it did in the modeling stage this approach could be applied to each family to try to improve the score. 

- Only one model was chosen with minimal tuning applied was used for each family. Different families could excel with different models, specifically the top selling families.

- The holidays dataset was neglected completely. Having this would likely significantly boost the predictions of families such as the liquor, wine, and beer family which displayed a very poor rmsle when performing cross validation.


